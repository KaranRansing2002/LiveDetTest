<!DOCTYPE html>
<html xmlns:th="http://www.thymeleaf.org">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Camera Feed Modal</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        #video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .modal-dialog {
            max-width: 100%;
            width: 100%;
            height: 100%;
            margin: 0;
            display: flex;
            justify-content: center;
        }

        .modal-content {
            width: 550px;
            height: 720px;
            border-radius: 0;
            overflow: hidden;
        }

        #overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 2;
            pointer-events: none;
            /* Prevent interactions with the canvas */
        }
    </style>
</head>

<body>

    <div class="container text-center" style="margin-top: 20%;">
        <select id="cameraSelect" class="form-select mb-3" aria-label="Camera Select" style="display: none;">
            <option value="user">Front Camera</option>
            <option value="environment">Back Camera</option>
        </select>
        motion parallax
        <button id="openCameraBtn" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#cameraModal"
            disabled>
            Open Camera
        </button>
        <p id="loadingStatus">Loading models...</p>
    </div>

    <div>
        <img style="border:2px solid red; margin-right: 4px;" id="myimage" src="" />
        <img style="border:2px solid red;" id="myimage1" src="" />
    </div>
    <!-- Modal -->
    <div class="modal fade" id="cameraModal" tabindex="-1" aria-labelledby="cameraModalLabel" aria-hidden="true"
        style="padding: 0;">
        <div class="modal-dialog modal-dialog-centered">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title" id="cameraModalLabel">Camera Feed</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body" style="position: relative; padding: 0px;">
                    <video id="video" autoplay></video>
                    <canvas id="overlay"></canvas>
                </div>
            </div>
        </div>
    </div>

    <!-- Bootstrap JS with Popper.js -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="./face-api.min.js"></script> <!-- Ensure this path is correct -->
    <script>

        const video = document.getElementById('video');
        const overlay = document.getElementById('overlay');
        const context = overlay.getContext('2d');
        const openCameraBtn = document.getElementById('openCameraBtn');
        const loadingStatus = document.getElementById('loadingStatus');
        const cameraSelect = document.getElementById('cameraSelect');
        const myimage = document.getElementById('myimage');
        const myimage1 = document.getElementById('myimage1');
        let eyeClosed = false;
        let coolDownTime = 0;

        async function loadModels() {
            try {
                console.log("Loading models...");
                // Log each model loading step
                await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
                console.log("tinyFaceDetector model loaded.");

                await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
                console.log("faceLandmark68Net model loaded.");

                openCameraBtn.disabled = false; // Enable the button once models are loaded
                loadingStatus.textContent = 'Models loaded. You can now open the camera!';
            } catch (error) {
                loadingStatus.textContent = 'Error loading models. Check your paths.';
                console.error("Error loading models:", error);
            }
        }

        function isMobileDevice() {
            return /Mobi|Android/i.test(navigator.userAgent); // Basic mobile detection
        }
        var faceDetectionInterval;

        function startCamera() {

            const selectedCamera = isMobileDevice() ? cameraSelect.value : 'environment'; // Default to back camera for non-mobile
            const constraints = {
                video: {
                    width: 1280,
                    height: 960,
                    facingMode: selectedCamera,
                }
            };

            if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
                navigator.mediaDevices.getUserMedia(constraints)
                    .then(function (stream) {
                        video.srcObject = stream;
                        video.play();
                        console.log("Camera started.");

                        //checking the camera actual height and width
                        const videoTrack = stream.getVideoTracks()[0];
                        const settings = videoTrack.getSettings();
                        const cameraWidth = settings.width;
                        const cameraHeight = settings.height;
                        console.log(`Camera resolution: ${cameraWidth}x${cameraHeight}`);


                        // Dynamically adjust canvas size to match video
                        video.onloadedmetadata = () => {
                            overlay.width = video.videoWidth;
                            overlay.height = video.videoHeight;

                            // Start face detection loop
                            faceDetectionInterval = setInterval(detectSingleFace, 200);
                        };
                    })
                    .catch(function (error) {
                        console.error("Error accessing camera:", error);
                    });
            } else {
                console.error("Browser does not support camera access.");
            }
        }
        let detection
        async function detectSingleFace() {
            try {
                // Detect a single face with landmarks
                detection = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks();

                if (detection && detection.landmarks) {
                    const box = detection.alignedRect.box; // Bounding box of the detected face
                    const essentialLandmarks = getEssentialLandmarks(detection.landmarks);

                    // Check if all essential landmarks are inside the guider
                    const areLandmarksInside = essentialLandmarks.every(({ name, position }) => {
                        const isInside = isPointInGuider(position.x, position.y);
                        if (!isInside) {
                            //console.log(`Outside guider: ${name} at (${position.x}, ${position.y})`);
                        }
                        return isInside;
                    });

                    // Check if face covers more than 70% of the guider
                    const doesFaceCoverGuider = doesFaceCoverEnough(box);

                    // Draw guider based on both conditions
                    const isFaceValid = areLandmarksInside && doesFaceCoverGuider;
                    drawFaceGuider(isFaceValid);

                    if (isFaceValid) {
                        //console.log(video.videoHeight, video.videoWidth);
                        checkBlink(detection.landmarks);
                    }
                } else {
                    drawFaceGuider(false);
                }
            } catch (error) {
                console.error("Error in face detection:", error);
            }
        }

        function calculateEAR(eye) {
            const A = Math.hypot(eye[1].x - eye[5].x, eye[1].y - eye[5].y); // Vertical distance between 1st and 5th landmarks
            const B = Math.hypot(eye[2].x - eye[4].x, eye[2].y - eye[4].y); // Vertical distance between 2nd and 4th landmarks
            const C = Math.hypot(eye[0].x - eye[3].x, eye[0].y - eye[3].y); // Horizontal distance between 0th and 3rd landmarks
            return (A + B) / (2.0 * C); // EAR formula
        }

        let nam = 0;
        let blinked = false;
        let inside = true; // Initially the face is inside the frame
        let h_ = 0;
        let prevh_ = 0;
        let opening = false;
        let opening1 = 1000;
        let opening2 = 1000;
        let lowest = 0;
        let image1 = 0, image2 = 0, image3 = 0;

        // Function to check blink and eye opening
        async function checkBlink(detection) {
            
            nam++; // Increase the counter for each check

            if (nam == 1 || nam == 2) {
                // Initialize/reset values on the first or second frame
                blinked = false;
                inside = false;
                h_ = prevh_; // Set h_ to prevh_ at the start
            }

            // Update images to simulate eye detection frames
            image3 = image1;
            image2 = image1;
            image1 = prevh_;

            // Track lowest height
            if (image1 < h_) {
                lowest = h_;
            }

            // Store the previous height for the next frame
            prevh_ = h_;

            // Get the eye landmarks from the detection object
            const leftEye = detection.landmarks.getLeftEye();
            const rightEye = detection.landmarks.getRightEye();

            // Calculate the height for the left and right eyes
            const leftEyeHeight = leftEye[4].y - leftEye[1].y; // Example: height of the left eye
            const rightEyeHeight = rightEye[4].y - rightEye[1].y; // Example: height of the right eye

            // We will choose the maximum height of the two eyes
            h_ = Math.max(leftEyeHeight, rightEyeHeight);

            console.log(h_,prevh_);
            // Eye opening logic
            if (h_ > prevh_) {
                opening2 = h_;
                if (prevh_ > image1) {
                    opening1 = prevh_;
                    if (opening1 > opening2) {
                        opening = true; // Eye opening detected
                    }
                }
            } else {
                opening1 = 1000;
                opening2 = 1000;
                opening = false;
            }

            // Blink detection logic based on height difference
            if (((prevh_ - h_) > 1 || (h_ - prevh_) > 1) && inside === true) {
                blinked = true; // Blink detected
            } else {
                // No blink detected, reset the flag
                blinked = false;
            }

            // Optionally, you can log the results for testing purposes
            console.log(`Blink detected: ${blinked}, Eye Opening detected: ${opening}`);
        }


        // Helper to get essential landmarks
        function getEssentialLandmarks(landmarks) {
            return [
                { name: 'leftEye', position: landmarks.getLeftEye()[0] },
                { name: 'rightEye', position: landmarks.getRightEye()[0] },
                { name: 'nose', position: landmarks.getNose()[0] },
                { name: 'mouth', position: landmarks.getMouth()[0] }
            ];
        }

        // Function to check if a point is within the face guider's bounds
        function isPointInGuider(x, y, box) {
            const guideHeightPercentage = (window.innerWidth <= 460) ? 0.3 : 0.55;
            const guideWidthPercentage = (window.innerWidth <= 460) ? 0.3 : 0.35;

            const guideHeight = video.videoHeight * guideHeightPercentage;
            const guideWidth = video.videoWidth * guideWidthPercentage;
            const guideX = video.videoWidth / 2;
            const guideY = video.videoHeight / 2;

            const radiusX = guideWidth / 2;
            const radiusY = guideHeight / 2;

            const normalizedX = (x - guideX) / radiusX;
            const normalizedY = (y - guideY) / radiusY;

            return (normalizedX ** 2 + normalizedY ** 2) <= 1;
        }

        function doesFaceCoverEnough(box) {
            const guideHeightPercentage = (window.innerWidth <= 460) ? 0.3 : 0.55;
            const guideWidthPercentage = (window.innerWidth <= 460) ? 0.3 : 0.35;

            const guideHeight = video.videoHeight * guideHeightPercentage;
            const guideWidth = video.videoWidth * guideWidthPercentage;

            const faceArea = box.width * box.height;
            const guiderArea = guideWidth * guideHeight;

            return faceArea >= guiderArea * 0.6; // 70% threshold
        }

        // Function to draw the face guider
        function drawFaceGuider(isAllInGuider) {
            context.clearRect(0, 0, overlay.width, overlay.height);
            const guideLineColor = isAllInGuider ? '#00FF00' : 'red';

            const guideHeightPercentage = (window.innerWidth <= 460) ? 0.3 : 0.55;
            const guideWidthPercentage = (window.innerWidth <= 460) ? 0.3 : 0.35;

            const guideHeight = video.videoHeight * guideHeightPercentage;
            const guideWidth = video.videoWidth * guideWidthPercentage;
            const guideX = video.videoWidth / 2;
            const guideY = video.videoHeight / 2;

            context.strokeStyle = guideLineColor;
            context.lineWidth = 3;
            context.beginPath();
            context.ellipse(guideX, guideY, guideWidth / 1.3, guideHeight / 1.75, 0, 0, 2 * Math.PI);
            context.stroke();
        }

        function captureImageAtCameraResolution() {
            //Ensure video has loaded and has resolution data
            // if (video.videoWidth && video.videoHeight) {
            //     // Set canvas dimensions to match video (camera's native resolution)
            //     const canvas = document.createElement('canvas');
            //     canvas.width = video.videoWidth;
            //     canvas.height = video.videoHeight;

            //     // Draw the current frame from the video onto the canvas
            //     const ctx = canvas.getContext('2d');
            //     ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            //     // Get the image data URL (in full camera resolution)
            //     const imageDataUrl = canvas.toDataURL('image/png');
            //     //console.log("Captured Image URL:", imageDataUrl);

            //     // You can now use the imageDataUrl as needed
            //     myimage.src = imageDataUrl;
            //     return imageDataUrl;
            // } else {
            //     console.warn("Video is not ready for capturing yet.");
            //     return null;
            // }

            if (video.videoWidth && video.videoHeight) {
                myimage.src = captureAndResizeImage(video);
            } else {
                console.warn("Video is not ready for capturing yet.");
                return null;
            }
        }

        function captureAndResizeImage(videoElement, targetWidth = 550, targetHeight = 720) {
            // Step 1: Create a canvas at the camera's maximum resolution
            const maxWidth = videoElement.videoWidth;
            const maxHeight = videoElement.videoHeight;
            const highResCanvas = document.createElement("canvas");
            highResCanvas.width = maxWidth;
            highResCanvas.height = maxHeight;
            const highResCtx = highResCanvas.getContext("2d");

            // Draw the video frame onto the high-resolution canvas
            highResCtx.drawImage(videoElement, 0, 0, maxWidth, maxHeight);
            myimage1.src = highResCanvas.toDataURL("image/png");

            // Step 2: Create another canvas for resizing the image to target dimensions
            const targetCanvas = document.createElement("canvas");
            targetCanvas.width = targetWidth;
            targetCanvas.height = targetHeight;
            const targetCtx = targetCanvas.getContext("2d");

            // Calculate aspect ratios
            const sourceAspectRatio = maxWidth / maxHeight;
            const targetAspectRatio = targetWidth / targetHeight;

            let renderWidth, renderHeight, offsetX = 0, offsetY = 0;

            // Determine the dimensions to draw the high-res image onto the target canvas
            if (sourceAspectRatio > targetAspectRatio) {
                // Image is wider than target, crop width
                renderHeight = targetHeight;
                renderWidth = targetHeight * sourceAspectRatio;
                offsetX = (renderWidth - targetWidth) / 2;
            } else {
                // Image is taller than target, crop height
                renderWidth = targetWidth;
                renderHeight = targetWidth / sourceAspectRatio;
                offsetY = (renderHeight - targetHeight) / 2;
            }

            // Step 3: Draw the high-resolution image onto the target canvas with resizing
            targetCtx.drawImage(highResCanvas, -offsetX, -offsetY, renderWidth, renderHeight);

            // Step 4: Get the resized image as a data URL or Blob
            const resizedImageDataUrl = targetCanvas.toDataURL("image/png");  // Can also use .toBlob

            // Display or download the image as needed
            //console.log("Resized Image Data URL:", resizedImageDataUrl);
            return resizedImageDataUrl;
        }



        if (isMobileDevice()) {
            cameraSelect.style.display = 'block'; // Show the dropdown for mobile devices
        }

        loadModels();

        document.getElementById('cameraModal').addEventListener('shown.bs.modal', startCamera);

        document.getElementById('cameraModal').addEventListener('hidden.bs.modal', function () {
            //captureImageAtCameraResolution();
            const stream = video.srcObject;
            const tracks = stream.getTracks();
            tracks.forEach(track => track.stop());
            video.srcObject = null;
            clearInterval(faceDetectionInterval);
        });

        function closeModal() {
            // const stream = video.srcObject;
            // const tracks = stream.getTracks();
            // tracks.forEach(track => track.stop());
            // video.srcObject = null;
            // clearInterval(faceDetectionInterval);
            const modalElement = document.getElementById('cameraModal');
            const modalInstance = bootstrap.Modal.getInstance(modalElement) || new bootstrap.Modal(modalElement);
            modalInstance.hide();
        }

    </script>

</body>

</html>